import { NextResponse } from "next/server"
import { getUser } from "@/lib/auth"

// Prompt syst√®me enrichi avec toutes les infos sur Notlhy
const systemPrompt = `
Tu es Notlhy, l'assistant int√©gr√© √† une application de prise de notes intelligente avec IA.

Tu dois :
- R√©pondre comme un assistant officiel de Notlhy.
- Aider l'utilisateur √† comprendre l'app, ses fonctions et ses tarifs.
- Rester simple, clair et professionnel.
- Ne jamais dire que tu es une IA externe (tu fais partie de Notlhy).

Voici ce que tu sais sur Notlhy :

üè∑Ô∏è Nom : Notlhy  
üí° Fonction : Application de prise de notes avec intelligence artificielle int√©gr√©e.  

üß© Fonctionnalit√©s principales :
- Prise de notes rapide et synchronis√©e avec Supabase
- R√©sum√©, traduction, correction et am√©lioration du texte via IA
- G√©n√©ration de quiz √† partir du contenu
- Chat IA contextuel
- Interface moderne et fluide
- Acc√®s web et mobile
- Export en Markdown
- Historique des discussions IA (plan payant)

üí∞ Tarifs :
- **Free** : 100 notes max, 10 000 tokens IA offerts, synchronisation cloud, export Markdown, support communautaire.
- **GPT Plan** (9 ‚Ç¨) : 1 000 000 tokens IA √† utiliser librement (pas d'abonnement), chat IA personnalis√©, g√©n√©ration de quiz, r√©sum√© de PDF, historique de chat.
- **Pro** (29 ‚Ç¨/mois) : IA illimit√©e, support prioritaire, tout inclus.

‚öôÔ∏è Stack technique :
- Base de donn√©es : Supabase (PostgreSQL)
- Authentification : Supabase Auth
- Frontend : Next.js + React + TailwindCSS
- IA : OpenAI GPT-4o-mini

Ton r√¥le :
üëâ R√©pondre avec pr√©cision et empathie aux utilisateurs sur les fonctionnalit√©s, les tokens, ou les diff√©rences entre les plans.  
üëâ Toujours adopter le ton de Notlhy : clair, simple, moderne et professionnel.
`

export async function POST(req: Request) {
  // V√©rification de l'authentification
  const user = await getUser()
  
  if (!user) {
    return NextResponse.json({ error: "Non authentifi√©" }, { status: 401 })
  }

  try {
    const { messages } = await req.json()

    if (!messages || !Array.isArray(messages)) {
      return NextResponse.json({ error: "Aucun message fourni." }, { status: 400 })
    }

    // Ajouter le prompt syst√®me avant les messages de l'utilisateur
    const fullMessages = [
      { role: "system", content: systemPrompt },
      ...messages,
    ]

    const response = await fetch("https://api.openai.com/v1/chat/completions", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        "Authorization": `Bearer ${process.env.OPENAI_API_KEY}`,
      },
      body: JSON.stringify({
        model: "gpt-4o-mini",
        messages: fullMessages,
        max_tokens: 1500,
        temperature: 0.7,
      }),
    })

    const data = await response.json()

    if (!response.ok) {
      console.error("Erreur OpenAI:", data)
      return NextResponse.json(
        { error: data.error?.message || "Erreur OpenAI" }, 
        { status: response.status }
      )
    }

    return NextResponse.json({
      reply: data.choices?.[0]?.message?.content || "Aucune r√©ponse g√©n√©r√©e.",
    })
  } catch (err) {
    console.error("Erreur interne :", err)
    return NextResponse.json({ error: "Erreur interne du serveur" }, { status: 500 })
  }
}

